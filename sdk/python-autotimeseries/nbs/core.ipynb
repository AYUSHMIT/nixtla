{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11be6740-ff47-46d9-880d-19f8783f9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa167045-a44b-4b7b-b47d-8dd75975fd27",
   "metadata": {},
   "source": [
    "# AutoTimeSeries\n",
    "\n",
    "> Core implementation of Nixtla SDK, `autotimeseries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84c7ce4-91cb-4964-8930-255cc7f84ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0ef1d1-90c9-454b-8b68-f219bc9d9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6fc08f-2fd1-4e0d-adc4-4b5bf2868985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27ec6e2a-0f4c-4b25-9362-1b46befd1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AutoTS:\n",
    "    \"\"\"Compute time series features at scale.\n",
    "    Send an email to fede.garza.ramirez@gmail.com to request access.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket_name: str\n",
    "        Bucket name.\n",
    "    api_id: str\n",
    "        API identifier. \n",
    "    api_key: str\n",
    "        API key.\n",
    "    aws_access_key_id: str\n",
    "        AWS ACCESS KEY ID.\n",
    "    aws_secret_access_key: str\n",
    "        AWS SECRET ACCESS KEY.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str,  \n",
    "                 api_id: str, api_key: str,\n",
    "                 aws_access_key_id: str,\n",
    "                 aws_secret_access_key: str) -> 'AutoTS':\n",
    "        self.bucket_name = bucket_name\n",
    "        self.invoke_url = f'https://{api_id}.execute-api.us-east-1.amazonaws.com/main'\n",
    "        self.api_key = api_key\n",
    "        self.aws_access_key_id = aws_access_key_id\n",
    "        self.aws_secret_access_key = aws_secret_access_key\n",
    "    \n",
    "    def _parse_response(self, response: str) -> Dict:\n",
    "        \"\"\"Parses responde.\"\"\"\n",
    "        try:\n",
    "            resp = json.loads(response)\n",
    "        except Exception as e:\n",
    "            raise Exception(response)\n",
    "    \n",
    "        return resp\n",
    "    \n",
    "    def _progress(self, size: float):\n",
    "        \"\"\"Progress bar based on size.\"\"\"\n",
    "        downloaded = 0\n",
    "\n",
    "        def progress(chunk):\n",
    "            nonlocal downloaded\n",
    "            downloaded += chunk\n",
    "            done = int(50 * downloaded / size)\n",
    "            sys.stdout.write(f'\\r[{\"=\" * done}{\" \" * (50-done)}]')\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        return progress\n",
    "    \n",
    "    def upload_to_s3(self, file: str, force_upload: bool = False) -> str:\n",
    "        \"\"\"Uploads file to s3.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file: str\n",
    "            Local file.\n",
    "        force_upload: bool\n",
    "            If True forces uploading even if the file already exists.\n",
    "        \"\"\"\n",
    "        if self.aws_access_key_id is None:\n",
    "            raise Exception(\n",
    "                'To use `upload_to_s3` you need to pass '\n",
    "                '`aws_access_key_id` and '\n",
    "                '`aws_secret_access_key`'\n",
    "            )\n",
    "            \n",
    "        filename = file.split('/')[-1]\n",
    "        \n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=self.aws_access_key_id,\n",
    "            aws_secret_access_key=self.aws_secret_access_key\n",
    "        )\n",
    "        # Check if exists\n",
    "        if not force_upload:\n",
    "            try:\n",
    "                session = boto3.Session(\n",
    "                    aws_access_key_id=self.aws_access_key_id,\n",
    "                    aws_secret_access_key=self.aws_secret_access_key\n",
    "                )\n",
    "                \n",
    "                session.resource('s3').Object(self.bucket_name, filename).load()\n",
    "            except botocore.exceptions.ClientError as e:\n",
    "                if e.response['Error']['Code'] != \"404\":\n",
    "                    raise e\n",
    "            else:\n",
    "                # The object does exist\n",
    "                return filename\n",
    "        \n",
    "        # Progress bar\n",
    "        size = os.stat(file).st_size\n",
    "        progress_bar = self._progress(size)\n",
    "        \n",
    "        # Uploading file\n",
    "        s3.upload_file(file, self.bucket_name, filename, Callback=progress_bar)\n",
    "            \n",
    "        return filename \n",
    "    \n",
    "    def download_from_s3(self, filename: str, filename_output: Optional[str] = None) -> str:\n",
    "        \"\"\"Downloads file from s3.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str\n",
    "            File to download.\n",
    "        filename_output: str\n",
    "            Filename output.\n",
    "        \"\"\"\n",
    "        if self.aws_access_key_id is None:\n",
    "            raise Exception(\n",
    "                'To use `download_from_s3` you need to pass '\n",
    "                '`aws_access_key_id` and '\n",
    "                '`aws_secret_access_key`'\n",
    "            )\n",
    "        \n",
    "        s3 = boto3.client('s3',\n",
    "                          aws_access_key_id=self.aws_access_key_id,\n",
    "                          aws_secret_access_key=self.aws_secret_access_key)\n",
    "        \n",
    "        # Progress bar\n",
    "        meta_data = s3.head_object(Bucket=self.bucket_name, Key=filename)\n",
    "        size = int(meta_data.get('ContentLength', 0))\n",
    "        progress_bar = self._progress(size)\n",
    "\n",
    "        # Downloading file\n",
    "        s3.download_file(self.bucket_name, filename, \n",
    "                         filename if filename_output is None else filename_output, \n",
    "                         Callback=progress_bar)\n",
    "        \n",
    "    def _tsfeatures(self, filename: str, freq: int, \n",
    "                    kind: str = 'static',\n",
    "                    unique_id_column: str = 'unique_id',\n",
    "                    ds_column: str = 'ds',\n",
    "                    y_column: str = 'y') -> Dict:\n",
    "        \"\"\"Calculates features from S3 URL.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str\n",
    "            Filename of the dataset.\n",
    "            The dataset should contain at least three columns:\n",
    "                - Time series identifier.\n",
    "                - Time identifier.\n",
    "                - Target identifier.\n",
    "        freq: int\n",
    "            Frequency of the time series.\n",
    "        kind: str\n",
    "            Kind of features. Static returns features for each time series.\n",
    "            temporal returns for each ds and each time series.\n",
    "        unique_id_column: str\n",
    "            Column name identifying each time series.\n",
    "        ds_column: str\n",
    "            Column name identifying each time stamp.\n",
    "        y_column: str\n",
    "            Column name identifying the target variable.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        [1] The dataset should contain time series of the same frequency.\n",
    "        \"\"\"\n",
    "        query = dict(\n",
    "            s3_args=dict(\n",
    "                s3_url=f's3://{self.bucket_name}',\n",
    "            ), \n",
    "            args = dict(\n",
    "                filename=filename,\n",
    "                kind=kind,\n",
    "                freq=freq,\n",
    "                unique_id_column=unique_id_column,\n",
    "                ds_column=ds_column,\n",
    "                y_column=y_column,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        resp = requests.post(f'{self.invoke_url}/tsfeatures', \n",
    "                             headers={'x-api-key': self.api_key},\n",
    "                             data=json.dumps(query))\n",
    "        \n",
    "        return self._parse_response(resp.text)\n",
    "    \n",
    "    def tsfeatures_temporal(self, filename: str, freq: int, \n",
    "                            unique_id_column: str = 'unique_id',\n",
    "                            ds_column: str = 'ds',\n",
    "                            y_column: str = 'y') -> pd.DataFrame:\n",
    "        \"\"\"Calculates temporal features from S3 URL.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str\n",
    "            Filename of the dataset.\n",
    "            The dataset should contain at least three columns:\n",
    "                - Time series identifier.\n",
    "                - Time identifier.\n",
    "                - Target identifier.\n",
    "        freq: int\n",
    "            Frequency of the time series.\n",
    "        unique_id_column: str\n",
    "            Column name identifying each time series.\n",
    "        ds_column: str\n",
    "            Column name identifying each time stamp.\n",
    "        y_column: str\n",
    "            Column name identifying the target variable.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        [1] The dataset should contain time series of the same frequency.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._tsfeatures(s3_uri=s3_uri,\n",
    "                                kind='temporal',\n",
    "                                freq=freq,\n",
    "                                unique_id_column=unique_id_column,\n",
    "                                ds_column=ds_column,\n",
    "                                y_column=y_column)\n",
    "    \n",
    "    def tsfeatures_static(self, filename: str, freq: int, \n",
    "                          unique_id_column: str = 'unique_id',\n",
    "                          ds_column: str = 'ds',\n",
    "                          y_column: str = 'y') -> Dict:\n",
    "        \"\"\"Calculates static features from S3 URL.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str\n",
    "            Filename of the dataset.\n",
    "            The dataset should contain at least three columns:\n",
    "                - Time series identifier.\n",
    "                - Time identifier.\n",
    "                - Target identifier.\n",
    "        freq: int\n",
    "            Frequency of the time series.\n",
    "        unique_id_column: str\n",
    "            Column name identifying each time series.\n",
    "        ds_column: str\n",
    "            Column name identifying each time stamp.\n",
    "        y_column: str\n",
    "            Column name identifying the target variable.\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        [1] The dataset should contain time series of the same frequency.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._tsfeatures(s3_uri=s3_uri,\n",
    "                                kind='static',\n",
    "                                freq=freq,\n",
    "                                unique_id_column=unique_id_column,\n",
    "                                ds_column=ds_column,\n",
    "                                y_column=y_column)\n",
    "    \n",
    "    def calendartsfeatures(self, filename: str, country: str, \n",
    "                           events: Optional[Union[str, Dict[str, List[str]]]] = None,\n",
    "                           unique_id_column: str = 'unique_id',\n",
    "                           ds_column: str = 'ds',\n",
    "                           y_column: str = 'y') -> Dict:\n",
    "        \"\"\"Calculates calendar features from S3 URL.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str\n",
    "            filename of the dataset.\n",
    "            The dataset should contain at least three columns:\n",
    "                - Time series identifier.\n",
    "                - Time identifier.\n",
    "                - Target identifier.\n",
    "        country: str\n",
    "            Country to calculate calendar features from.\n",
    "        events: Union[str, Dict[str, List[str]]]\n",
    "            - str file\n",
    "                String file of events with the format\n",
    "                event1=fecha1,fecha2/event2=fecha1,fecha2,fecha3\n",
    "                allocated in the bucket.\n",
    "            - Dictionary of events.\n",
    "                The key defines the name of the event and \n",
    "                the value is a list of the dates to consider.\n",
    "                Ej. {'important_event_1': ['2010-02-05', '2011-02-07', '2012-02-08', '2013-02-09'],\n",
    "                     'important_event_2': ['2010-03-14', '2011-03-12', '2012-03-16', '2013-03-18'],}\n",
    "                The format of each date should be 'YYYY-MM-DD'.\n",
    "        unique_id_column: str\n",
    "            Column name identifying each time series.\n",
    "        ds_column: str\n",
    "            Column name identifying each time stamp.\n",
    "        y_column: str\n",
    "            Column name identifying the target variable.\n",
    "        \"\"\"\n",
    "        query = dict(\n",
    "            s3_args=dict(\n",
    "                s3_url=f's3://{self.bucket_name}',\n",
    "            ), \n",
    "            args = dict(\n",
    "                filename=filename,\n",
    "                country=country,\n",
    "                unique_id_column=unique_id_column,\n",
    "                ds_column=ds_column,\n",
    "                y_column=y_column,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if events is not None:\n",
    "            if isinstance(events, str):\n",
    "                query['args']['events'] = events\n",
    "            else:\n",
    "                str_events = [f'{key}={\",\".join(value)}' for key, value in events.items()]\n",
    "                str_events = '/'.join(str_events)\n",
    "                query['args']['events'] = str_events\n",
    "        \n",
    "        resp = requests.post(f'{self.invoke_url}/calendartsfeatures', \n",
    "                             headers={'x-api-key': self.api_key},\n",
    "                             data=json.dumps(query))\n",
    "        \n",
    "        return self._parse_response(resp.text)\n",
    "    \n",
    "    def tsforecast(self, \n",
    "                   filename_target: str, \n",
    "                   freq: str,\n",
    "                   seasonality: int,\n",
    "                   horizon: int,\n",
    "                   unique_id_column: str = 'unique_id',\n",
    "                   ds_column: str = 'ds',\n",
    "                   y_column: str = 'y') -> Dict:\n",
    "        \"\"\"Calculates forecast.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename_target: str\n",
    "            Filename of the target dataset.\n",
    "            The dataset should contain at least three columns:\n",
    "                - Time series identifier.\n",
    "                - Time identifier.\n",
    "                - Target identifier.\n",
    "        freq: str\n",
    "            Frequency of the data.\n",
    "            Only 'D' supported.\n",
    "        horizon: int\n",
    "            Forcast horizon.\n",
    "        unique_id_column: str\n",
    "            Column name identifying each time series.\n",
    "        ds_column: str\n",
    "            Column name identifying each time stamp.\n",
    "        y_column: str\n",
    "            Column name identifying the target variable.\n",
    "        \"\"\"\n",
    "        query = dict(\n",
    "            s3_args=dict(\n",
    "                s3_url=f's3://{self.bucket_name}',\n",
    "            ), \n",
    "            data_args = dict(\n",
    "                filename=filename_target,\n",
    "                freq=freq,\n",
    "                seasonality=seasonality,\n",
    "                horizon=horizon,\n",
    "                unique_id_column=unique_id_column,\n",
    "                ds_column=ds_column,\n",
    "                y_column=y_column,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        resp = requests.post(f'{self.invoke_url}/tsforecast', \n",
    "                             headers={'x-api-key': self.api_key},\n",
    "                             data=json.dumps(query))\n",
    "        \n",
    "        return self._parse_response(resp.text)\n",
    "    \n",
    "    def tsbenchmarks(self, \n",
    "                     filename: str,\n",
    "                     dataset: str,\n",
    "                     unique_id_column: str = 'unique_id',\n",
    "                     ds_column: str = 'ds',\n",
    "                     y_column: str = 'y') -> Dict:\n",
    "        \"\"\"Calculates performance based on filename and dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str\n",
    "            Filename of the dataset.\n",
    "            The dataset should contain at least three columns:\n",
    "                - Time series identifier.\n",
    "                - Time identifier.\n",
    "                - Target identifier.\n",
    "        dataset: str\n",
    "            Target dataset.\n",
    "        unique_id_column: str\n",
    "            Column name identifying each time series.\n",
    "        ds_column: str\n",
    "            Column name identifying each time stamp.\n",
    "        y_column: str\n",
    "            Column name identifying the target variable.\n",
    "        \"\"\"\n",
    "        query = dict(\n",
    "            s3_args=dict(\n",
    "                s3_url=f's3://{self.bucket_name}',\n",
    "            ), \n",
    "            args = dict(\n",
    "                filename=filename,\n",
    "                dataset=dataset,\n",
    "                unique_id_column=unique_id_column,\n",
    "                ds_column=ds_column,\n",
    "                y_column=y_column,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        resp = requests.post(f'{self.invoke_url}/tsbenchmarks', \n",
    "                             headers={'x-api-key': self.api_key},\n",
    "                             data=json.dumps(query))\n",
    "        \n",
    "        return self._parse_response(resp.text)\n",
    "    \n",
    "    \n",
    "    def get_status(self, job_id: str) -> Dict:\n",
    "        \"\"\"Gets job status.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        job_id: str\n",
    "            ID job from requests.\n",
    "        \"\"\"\n",
    "        resp = requests.get(f'{self.invoke_url}/jobs/', \n",
    "                            params={'job_id': job_id},\n",
    "                            headers={'x-api-key': self.api_key})\n",
    "        \n",
    "        output = self._parse_response(resp.text)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def check_progress_logs(self, job_id: str, sleep_time: int = 45) -> Dict:\n",
    "        \"\"\"Waits untile `job_id` is done and prints logs.\"\"\"\n",
    "        sys.stdout.write(\n",
    "            \"\"\"\n",
    "            @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "            @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "            @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "            @@..*@@@@@@@.@@@@@@@@/.@@@@@@@@..@@@@@..@@@@@@..........@@@@@@..@@@@@@@@@@@@@@@@@.../@@@@@@@\n",
    "            @@.@*.(@@@@@.@@@@@@@@/.@@@@@@@@@@..@..@@@@@@@@@@@@..@@@@@@@@@@..@@@@@@@@@@@@@@@@..@@.,@@@@@@\n",
    "            @@.@@@,./@@@.@@@@@@@@/.@@@@@@@@@@@...@@@@@@@@@@@@@..@@@@@@@@@@..@@@@@@@@@@@@@@@.*@@@@..@@@@@\n",
    "            @@.@@@@@/.(@.@@@@@@@@/.@@@@@@@@@*.@@&.,@@@@@@@@@@@..@@@@@@@@@@..@@@@@@@@@@@@@@..........@@@@\n",
    "            @@.@@@@@@@...@@@@@@@@/.@@@@@@@(./@@@@@..&@@@@@@@@@..@@@@@@@@@@......@@@@@@@@/.@@@@@@@@@,.@@@\n",
    "            @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "            @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "            \n",
    "            Have some coffee while you wait...\n",
    "               ( (\n",
    "                ) )\n",
    "              ........\n",
    "              |      |]\n",
    "              \\      /    \n",
    "               `----'\n",
    "            \"\"\"\n",
    "        )\n",
    "        sys.stdout.write(f'\\nSetting up Nixtla infrastructure (this will take up 5 mins)...\\n')\n",
    "        \n",
    "        idx_logs = 0\n",
    "        in_progress = True\n",
    "        while in_progress:\n",
    "            resp = self.get_status(job_id)\n",
    "            status = resp['status']\n",
    "            logs = json.loads(resp['logs'])\n",
    "            \n",
    "            if status != 'InProgress' and not logs:\n",
    "                time.sleep(30)\n",
    "                resp = self.get_status(job_id)\n",
    "                status = resp['status']\n",
    "                logs = json.loads(resp['logs'])\n",
    "            \n",
    "            if logs:\n",
    "                #if logs != latest_logs:\n",
    "                for log in logs[idx_logs:]:\n",
    "                    sys.stdout.write(f'{log}\\n')\n",
    "                #latest_logs = logs\n",
    "                idx_logs = len(logs)\n",
    "            \n",
    "            in_progress = status == 'InProgress'\n",
    "            \n",
    "            time.sleep(sleep_time)\n",
    "                        \n",
    "        return status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88585e94-84b6-407e-ae32-e04ab220a940",
   "metadata": {},
   "source": [
    "# Time Series as a Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7418cbf7-6a2f-4af1-81a3-889420d75a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "autotimeseries = AutoTS(bucket_name=os.environ['BUCKET_NAME'],\n",
    "                        api_id=os.environ['API_ID'], \n",
    "                        api_key=os.environ['API_KEY'],\n",
    "                        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'], \n",
    "                        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb907751-1338-43ed-9f90-d29de8291c57",
   "metadata": {},
   "source": [
    "## M5 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8813307-5a34-428e-9a2e-adabd09d4f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================]"
     ]
    }
   ],
   "source": [
    "filename_target = autotimeseries.upload_to_s3('../data/target.parquet')\n",
    "filename_static = autotimeseries.upload_to_s3('../data/static.parquet')\n",
    "filename_temporal = autotimeseries.upload_to_s3('../data/temporal.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10adf013-574a-4e0d-857e-6111311a8fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target-1.parquet\n",
      "static.parquet\n",
      "temporal.parquet\n"
     ]
    }
   ],
   "source": [
    "print(filename_target, filename_static, filename_temporal, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad5e09-41b2-4841-8886-4d566486d91d",
   "metadata": {},
   "source": [
    "Each time series of the uploaded datasets is defined by the column `item_id`. Meanwhile the time column is defined by `timestamp` and the target column by `demand`. We need to pass this arguments to each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0acd4d63-ead0-4828-8e0f-38e37d45f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = dict(unique_id_column='item_id',\n",
    "               ds_column='timestamp',\n",
    "               y_column='demand')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd81b1-46f7-46b2-8bf2-c4a3161091ca",
   "metadata": {},
   "source": [
    "## Computing calendar features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0691ab58-75f9-4692-bf05-a8807bc05e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_calendar = autotimeseries.calendartsfeatures(filename=filename_temporal,\n",
    "                                                      country='USA',\n",
    "                                                      **columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f0e16465-ed3e-4ad6-99a5-1c3994323313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_job': '11ab846d-b2b8-42a9-bd91-787668cf292d',\n",
       " 'dest_url': 's3://nixtla-user-test/calendar-features.csv',\n",
       " 'status': 200,\n",
       " 'message': 'Check job status at GET /jobs/{job_id}'}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18825da4-be6e-4cea-ac2a-cfdb215f1778",
   "metadata": {},
   "source": [
    "To check the status of your job use the method `get_status`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5720a99f-451e-408f-9f93-481551c61b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'InProgress', 'processing_time_seconds': 0, 'logs': '[]'}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autotimeseries.get_status(response_calendar['id_job'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a64bd-69b6-4b9e-af9c-bea1553e4824",
   "metadata": {},
   "source": [
    "If you want to use the result of your job, you have to wait until its status is `Completed`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f3d4a-1f34-4094-91d2-5b4fc5a73737",
   "metadata": {},
   "source": [
    "## Make forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ad3e2f4-51cb-464c-90a8-e4407ee8d4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_job': '736ffa02-763d-4554-bcea-8a62a7081a70',\n",
       " 'dest_url': 's3://m5example/forecasts.csv',\n",
       " 'status': 200,\n",
       " 'message': 'Check job status at GET /jobs/{job_id}'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a36f4-2965-4b46-935a-b13b4a4e9b14",
   "metadata": {},
   "source": [
    "## Download forecasts from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f8b06-9470-4fba-adde-9ff1b148678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================]"
     ]
    }
   ],
   "source": [
    "autotimeseries.download_from_s3(filename='forecasts.csv', filename_output='../data/forecasts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db631a-0ddc-4f26-89f0-64e92527f974",
   "metadata": {},
   "source": [
    "## Evaluate performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901fa0e-db37-413e-9665-975d66026969",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_benchmarks = autotimeseries.tsbenchmarks(filename='forecasts.csv',\n",
    "                                                  dataset='M5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d1424-637f-4d71-9497-2921fbc5558a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_job': '7dbd525e-3fa9-416a-86ef-fbdadda129a3',\n",
       " 'dest_url': 's3://nixtla-user-test/benchmarks.csv',\n",
       " 'status': 200,\n",
       " 'message': 'Check job status at GET /jobs/{job_id}'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a613352-a7e5-4e1f-8306-3c274cd22003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully decompressed data/m5/datasets/m5.zip\n",
      "Successfully downloaded m5.zip, 50219189, bytes.\n",
      "Decompressing zip file...\n",
      "Reading file...\n",
      "File readed.\n",
      "Computing metrics...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'Completed', 'processing_time_seconds': 348}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autotimeseries.get_status(response_benchmarks['id_job'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ee8ac-c085-4530-91af-dae8608b965b",
   "metadata": {},
   "source": [
    "### Download evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218213a-e93a-47ae-a53a-8bfc3c0881d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "autotimeseries.download_from_s3(filename='benchmarks.csv', filename_output='../data/benchmarks.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
