{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp distributed.timegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fugue\n",
    "import fugue.api as fa\n",
    "from fugue.execution.factory import make_execution_engine\n",
    "from triad import Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _DistributedTimeGPT:\n",
    "\n",
    "    def forecast(\n",
    "            self,\n",
    "            token: str,\n",
    "            df: fugue.AnyDataFrame,\n",
    "            h: int,\n",
    "            freq: Optional[str] = None,    \n",
    "            id_col: str = 'unique_id',\n",
    "            time_col: str = 'ds',\n",
    "            target_col: str = 'y',\n",
    "            X_df: Optional[pd.DataFrame] = None,\n",
    "            level: Optional[List[Union[int, float]]] = None,\n",
    "            finetune_steps: int = 0,\n",
    "            clean_ex_first: bool = True,\n",
    "            validate_token: bool = False,\n",
    "            add_history: bool = False,\n",
    "            date_features: Union[bool, List[str]] = False,\n",
    "            date_features_to_one_hot: Union[bool, List[str]] = True,\n",
    "            num_partitions: Optional[int] = None,\n",
    "        ) -> fugue.AnyDataFrame:\n",
    "        kwargs = dict(\n",
    "            h=h,\n",
    "            freq=freq,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "            finetune_steps=finetune_steps,\n",
    "            clean_ex_first=clean_ex_first,\n",
    "            validate_token=validate_token,\n",
    "            add_history=add_history,\n",
    "            date_features=date_features,\n",
    "            date_features_to_one_hot=date_features_to_one_hot,\n",
    "        )\n",
    "        if id_col not in fa.get_column_names(df):\n",
    "            raise Exception(\n",
    "                'Distributed environment is meant to forecasts '\n",
    "                'multiple time series at once. You did not provide '\n",
    "                'an identifier for each time series.'\n",
    "            )\n",
    "        schema = self._get_forecast_schema(id_col=id_col, time_col=time_col, level=level)\n",
    "        engine = make_execution_engine(infer_by=[df])\n",
    "        if num_partitions is None:\n",
    "            num_partitions = engine.get_current_parallelism()\n",
    "        partition = dict(by=id_col, num=num_partitions, algo='coarse')\n",
    "        if X_df is not None:\n",
    "            raise Exception(\n",
    "                'Exogenous variables not supported for '\n",
    "                'distributed environments yet. '\n",
    "                'Please rise an issue at https://github.com/Nixtla/nixtla/issues/new '\n",
    "                'requesting the feature.'\n",
    "            )\n",
    "        fcst_df = fa.transform(\n",
    "            df,\n",
    "            self._forecast,\n",
    "            params=dict(token=token, kwargs=kwargs,),\n",
    "            schema=schema,\n",
    "            engine=engine,\n",
    "            partition=partition,\n",
    "            as_fugue=True,\n",
    "        )\n",
    "        return fa.get_native_as_df(fcst_df)\n",
    "\n",
    "    def _instantiate_timegpt(self, token: str):\n",
    "        from nixtlats.timegpt import _TimeGPT\n",
    "        timegpt = _TimeGPT(token=token)\n",
    "        return timegpt\n",
    "\n",
    "    def _forecast(\n",
    "            self, \n",
    "            df: pd.DataFrame, \n",
    "            token: str,\n",
    "            kwargs,\n",
    "        ) -> pd.DataFrame:\n",
    "        timegpt = self._instantiate_timegpt(token)\n",
    "        return timegpt._forecast(df=df, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_forecast_schema(id_col, time_col, level):\n",
    "        schema = f'{id_col}:string,{time_col}:datetime,TimeGPT:double'\n",
    "        if level is not None:\n",
    "            level = sorted(level)\n",
    "            schema = f'{schema},{\",\".join([f\"TimeGPT-lo-{lv}:double\" for lv in reversed(level)])}'\n",
    "            schema = f'{schema},{\",\".join([f\"TimeGPT-hi-{lv}:double\" for lv in level])}'\n",
    "        return Schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "\n",
    "from fastcore.test import test_eq\n",
    "from dotenv import load_dotenv\n",
    "from utilsforecast.data import generate_series\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_forecast(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        horizon: int = 12,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **fcst_kwargs,\n",
    "    ):\n",
    "    fcst_df = distributed_timegpt.forecast(\n",
    "        token=os.environ['TIMEGPT_TOKEN'], \n",
    "        df=df, \n",
    "        h=horizon,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs,\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    test_eq(n_series * 12, len(fcst_df))\n",
    "    cols = fcst_df.columns.to_list()\n",
    "    exp_cols = [id_col, time_col, 'TimeGPT']\n",
    "    if 'level' in fcst_kwargs:\n",
    "        level = sorted(fcst_kwargs['level'])\n",
    "        exp_cols.extend([f'TimeGPT-lo-{lv}' for lv in reversed(level)])\n",
    "        exp_cols.extend([f'TimeGPT-hi-{lv}' for lv in level])\n",
    "    test_eq(cols, exp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_same_results_num_partitions(\n",
    "        df: fugue.AnyDataFrame, \n",
    "        horizon: int = 12, \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        **fcst_kwargs,\n",
    "    ):\n",
    "    fcst_df = distributed_timegpt.forecast(\n",
    "        token=os.environ['TIMEGPT_TOKEN'], \n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=1,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df = fa.as_pandas(fcst_df)\n",
    "    fcst_df_2 = distributed_timegpt.forecast(\n",
    "        token=os.environ['TIMEGPT_TOKEN'], \n",
    "        df=df, \n",
    "        h=horizon, \n",
    "        num_partitions=2,\n",
    "        id_col=id_col,\n",
    "        time_col=time_col,\n",
    "        **fcst_kwargs\n",
    "    )\n",
    "    fcst_df_2 = fa.as_pandas(fcst_df_2)\n",
    "    pd.testing.assert_frame_equal(\n",
    "        fcst_df.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "        fcst_df_2.sort_values([id_col, time_col]).reset_index(drop=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_dataframe(df: fugue.AnyDataFrame):\n",
    "    test_forecast(df, num_partitions=1)\n",
    "    test_forecast(df, level=[90, 80], num_partitions=1)\n",
    "    test_same_results_num_partitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_dataframe_diff_cols(df: fugue.AnyDataFrame, id_col: str = 'id_col', time_col: str = 'time_col'):\n",
    "    test_forecast(df, id_col=id_col, time_col=time_col, num_partitions=1)\n",
    "    test_forecast(df, id_col=id_col, time_col=time_col, level=[90, 80], num_partitions=1)\n",
    "    test_same_results_num_partitions(df, id_col=id_col, time_col=time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "distributed_timegpt = _DistributedTimeGPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "n_series = 4\n",
    "horizon = 7\n",
    "\n",
    "series = generate_series(n_series)\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "\n",
    "series_diff_cols = series.copy()\n",
    "series_diff_cols = series_diff_cols.rename(columns={'unique_id': 'id_col', 'ds': 'time_col'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark_df = spark.createDataFrame(series)\n",
    "spark_diff_cols_df = spark.createDataFrame(series_diff_cols)\n",
    "test_dataframe(spark_df)\n",
    "test_dataframe_diff_cols(spark_diff_cols_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "dask_df = dd.from_pandas(series, npartitions=2)\n",
    "dask_diff_cols_df = dd.from_pandas(series_diff_cols, npartitions=2)\n",
    "test_dataframe(dask_df)\n",
    "test_dataframe_diff_cols(dask_diff_cols_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import ray\n",
    "from ray.cluster_utils import Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ray_cluster = Cluster(\n",
    "    initialize_head=True,\n",
    "    head_node_args={\"num_cpus\": 2}\n",
    ")\n",
    "ray.init(address=ray_cluster.address, ignore_reinit_error=True)\n",
    "# add mock node to simulate a cluster\n",
    "mock_node = ray_cluster.add_node(num_cpus=2)\n",
    "ray_df = ray.data.from_pandas(series)\n",
    "ray_diff_cols_df = ray.data.from_pandas(series_diff_cols)\n",
    "test_dataframe(ray_df)\n",
    "test_dataframe_diff_cols(ray_diff_cols_df)\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
